---
title: "Comprehensive Evaluation of Predictive Models and Feature Engineering in Financial Forecasting."
author: "Yamuna Dhungana"
output: 
    pdf_document:
        latex_engine: xelatex
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning=F,message=F)
```

I created a model using the "Weekly" dataset and fitted it with the MclustDA function from the "mclust" library. My objective was to select the most appropriate model based on the Bayesian Information Criterion (BIC).

Subsequently, I calculated key metrics including the true positive rate, true negative rate, training error, and test error. These measurements are crucial for evaluating the model's performance and its ability to correctly classify data points as positive or negative.
    
```{r,echo=FALSE,warning=FALSE}
library(ISLR)
library(mclust)
data("Weekly")

ex <- Weekly[,-8]
new_data <- c(which(Weekly$Year==2009), which(Weekly$Year==2010))
test_data <- ex[new_data,]
train <- ex[-new_data,]

# fitting the model with the predictors that I had chosen which is lag2
X.train = as.data.frame(train[,3]) ## keeping only Lag2
class.train = train$Direction

model1 <- MclustDA(X.train, class.train)
summary(model1)


pred_train=predict(model1, newdata=train[,3])$classification
## function to get overall accuracy, TPR and TNR
TPR_TNR=function(con){
  accuracy = round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
}
# TPR_TNR(table(pred_train, class.train))
table_train <- TPR_TNR(table(pred_train, class.train))
colnames(table_train) <- "train_error"
table_train

# test error
predstest=predict(model1, newdata=test_data[,3])$classification
trueClass=test_data[,8]
# TPR_TNR(table(predstest,trueClass))
table_test <- TPR_TNR(table(predstest, trueClass))
colnames(table_test) <- "test_error"
table_test




#############################################

cat("## With all the variables")
X=as.data.frame(train[,-8]) 
model1a = MclustDA(X, class.train)
summary(model1a)
predstrain = predict(model1a, newdata=train[,-8])$classification
table_train2 <- TPR_TNR(table(predstrain, class.train))
colnames(table_train2) <- "train_error"
table_train2

predstest=predict(model1a, newdata=test_data[,-8])$classification
trueClass=test_data[,8]
table_test2 <- TPR_TNR(table(predstest,trueClass))
colnames(table_test2) <- "test_error"
table_test2



```
      
  
In the preceding analysis, I identified Lag2 as the most significant variable. Nevertheless, I conducted two separate model runs: one exclusively with Lag2 and the other encompassing all variables. My primary aim was to assess the performance of the model when considering all variables.

In the single-variable model, the model is characterized by variable variance, rendering it one-dimensional and applicable to two distinct groups. Conversely, in the model with all variables, the structure is described as ellipsoidal, demonstrating varying volume, shape, and orientation. Notably, the Bayesian Information Criterion (BIC) for the single-variable model is higher than that for the model employing all variables.

Furthermore, I generated tables for both the test and train datasets, encompassing these two model scenarios, to facilitate a comprehensive comparison.
    

  Now, I'm re-running the MclustDA analysis, but this time I'm specifying modelType = "EDDA". I'm going through the same process of selecting the best model based on the Bayesian Information Criterion (BIC). Additionally, I'll calculate the true positive rate, true negative rate, training error, and test error.
  
    
```{r,echo=FALSE,warning=FALSE}

X=as.data.frame(train[,-8])
class <- train$Direction
model2=MclustDA(X, class, modelType = "EDDA")
summary(model2)

## train error
preds.train=predict(model2, newdata=train[,-8])$classification
table_train3 <- TPR_TNR(table(preds.train,class))
colnames(table_train3) <- "train_error"
table_train3


## test error
predstest=predict(model2, newdata=test_data[,-8])$classification
trueClass=test_data[,8]
table_test3 <- TPR_TNR(table(predstest,trueClass))
colnames(table_test3) <- "test_error"
table_test3



```
     
 I attempted to fit the model using both all variables and a single variable. However, it's important to note that the single-variable model failed to converge. Subsequently, I created tables to summarize the train and test errors for these models.

Upon examining the Mclust documentation, I discovered that specifying "EDDA" as the model type enforces a single component in each class with the same covariance structure. This single component exhibited an ellipsoidal structure with equal orientation, denoted as VVE.

Now Comparing the results,
    
```{r,echo=FALSE,warning=FALSE}

library(knitr)
tablecom <- as.data.frame(cbind(table_train, table_test))
knitr::kable(tablecom, digits = 3,
             caption = "MsclustDA Test Accuracy with single variable")

tablecom2 <- as.data.frame(cbind(table_train2, table_test2,table_train3,table_test3))
knitr::kable(tablecom2, digits = 3,
             caption = "MsclustDA Test Accuracy with all variables")

tablecom2a <- as.data.frame(cbind(table_train3,table_test3))
knitr::kable(tablecom2a, digits = 3,
             caption = "MsclustDA with EDDA Test Accuracy with all variables")


# for my previous work
# Logistic regression

fit_log2 <- glm(Direction~ Lag2, data = train, family= binomial)
#summary(fit_log2)

do.confusion=function(Th.hold,model,data){
  preds=rep("Down",dim(data)[1])
  vals=predict(model,newdata=data,type="response")
  for(i in 1:dim(data)[1]){
    if(vals[i]>=Th.hold){
      preds[i]="Up"
    }
  }
  con=table(preds,data$Direction)
  accuracy = round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}
# Train error
table_train4 <- do.confusion(0.5,fit_log2,train)
colnames(table_train4) <- "Train_error"

# Test error
table_test4 <- do.confusion(0.5,fit_log2,test_data)
colnames(table_test4) <- "Test_error"

# Combined error for logistic error
tablecom3 <- as.data.frame(cbind(table_train4, table_test4))
knitr::kable(tablecom3, digits = 3,
             caption = "Logreg Accuracy measures with single variable")


# FOr lda
library(MASS)
fit_lda = lda(Direction ~ Lag2, data = train)

# For qda
fit_qda <- qda(Direction~Lag2,data=train)

confusion_lqda =function(model,data){
  preds=(predict(model,newdata=data,type="response"))$class
  vals=predict(model,newdata=data,type="response")
  con=table(preds,data$Direction)
  accuracy = (round(sum(preds==data$Direction)/dim(data)[1]*100,2))
  TPR = (round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  TNR = round(con[1,1]/(con[1,1]+con[2,1])*100,2)
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}
# Train error
table_train5 <- confusion_lqda(fit_lda, train)
colnames(table_train5) <- "Train_error"

# Test error
table_test5 <- confusion_lqda(fit_lda, test_data)
colnames(table_test5) <- "Test_error"

# Combined error for lda error
tablecom4 <- as.data.frame(cbind(table_train5, table_test5))
knitr::kable(tablecom4, digits = 3,
             caption = "LDA Accuracy measures with single variable")


#############################################################
# Train error
table_train6 <- confusion_lqda(fit_qda, train)
colnames(table_train6) <- "Train_error"

# Test error
table_test6 <- confusion_lqda(fit_qda, test_data)
colnames(table_test6) <- "Test_error"

# Combined error for Qda error
tablecom5 <- as.data.frame(cbind(table_train6, table_test6))
knitr::kable(tablecom5, digits = 3,
             caption = "QDA Accuracy measures with single variable")



###########################################################################

do.confusionknn =function(model,trues){
  con=table(model,trues)
  accuracy = (round(((con[1,1]+con[2,2])/sum(con))*100,2))
  TPR = (round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  TNR = round(con[1,1]/(con[1,1]+con[2,1])*100)
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}

attach(Weekly)
# head(Weekly)
k_tdata = (Year < 2009)
knn_train = as.matrix(Lag2[k_tdata])
knn_test = as.matrix(Lag2[!k_tdata])
train_class = Direction[k_tdata]

library(class)
fit_knn <- knn(knn_train, knn_test, cl=train_class, k = 1)


# Test error
table_test7 <- do.confusionknn(fit_knn, test_data$Direction)
colnames(table_test7) <- "Test_error"

# Combined error for Qda error
tablecom6 <- as.data.frame(cbind(table_test7))
knitr::kable(tablecom6, digits = 3,
             caption = "KNN Accuracy measures with single variable")


```
      
    
In this context, I've compiled tables summarizing the results from all the methods we applied, both in the current analysis and previous ones. A quick glance at these tables reveals a range of test data accuracy, which spans from 62.50% to 46.15%. Similarly, training accuracy varies between 64.47% and 50.0%.

It's worth noting that the logistic regression model, in particular, stands out as highly accurate. Furthermore, the Linear Discriminant Analysis (LDA) model also demonstrates a commendable accuracy rate.
    
    
    
In this stage of the analysis, I took the original model variables and created a new set of variables. I then fitted a model using `MclustDA` and replicated the previous steps. The objective was to assess whether these new variables led to an improvement in error rates when compared to the previous models.
    
    
```{r,echo=FALSE,warning=FALSE}
# formula1=Direction~Lag1+Lag2
# formula2=Direction~Lag1+Lag2+Lag1*Lag2
# formula3=Direction~Lag2+I(Lag2^2)

exx <- Weekly[,-8]
# Keeping only lag1 and lag2
X1 = as.data.frame(exx[,2:3])
lag12 <- (exx$Lag1 * ex$Lag2)
X2 = cbind(X1, lag12)
direc <- exx$Direction
Xsq = (exx$Lag2)^2
finaltab <- cbind(X2,Xsq,direc)

new_data2 <- c(which(Weekly$Year==2009), which(Weekly$Year==2010))
tst_data <- finaltab[new_data2,]
trn_data <- finaltab[-new_data2,]
trn.class <- trn_data$direc




## MclustDA
modd1 <- MclustDA(trn_data[,1:2], trn.class)
summary(modd1)

modd2 <- MclustDA(trn_data[,1:3], trn.class)
summary(modd2)

modd3 <- MclustDA(trn_data[,c(2,4)], trn.class)
summary(modd3)

###################################################

## function to get overall accuracy, TPR and TNR
TPR_TNR_iv=function(con){
  accuracy = round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
}
pred_train1 = predict(modd1, newdata=trn_data[,1:2])$classification
table_train_1 <- TPR_TNR_iv(table(pred_train1, trn.class))
colnames(table_train_1) <- "tr.error modd1"
#table_train_1

# test error
predstest=predict(modd1, newdata=tst_data[,1:2])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_1 <- TPR_TNR_iv(table(predstest, tClass))
colnames(table_test_1) <- "tt.error modd1"
#table_test_1



# For second mmodel
# Train error
pred_train2 = predict(modd2, newdata=trn_data[,1:3])$classification
table_train_2 <- TPR_TNR_iv(table(pred_train2, trn.class))
colnames(table_train_2) <- "tr.error modd2"
#table_train_2

# test error
predstest2=predict(modd2, newdata=tst_data[,1:3])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_2 <- TPR_TNR_iv(table(predstest2, tClass))
colnames(table_test_2) <- "tt.error modd2"
#table_test_2



# For third model
# Train error
pred_train3 = predict(modd3, newdata=trn_data[,c(2,4)])$classification
table_train_3 <- TPR_TNR_iv(table(pred_train3, trn.class))
colnames(table_train_3) <- "tr.error modd3"
#table_train_3

# test error
predstest3=predict(modd3, newdata=tst_data[,c(2,4)])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_3 <- TPR_TNR_iv(table(predstest3, tClass))
colnames(table_test_3) <- "tt.error modd3"
#table_test_3


tablecombined <- as.data.frame(cbind(table_train_1,table_test_1,table_train_2,table_test_2,
                                     table_train_3,table_test_3))
knitr::kable(tablecombined, digits = 3,
             caption = "Accuracy measures using MclustDA")




################################################################################

## MclustDAwith EDDA
edmod1 <- MclustDA(trn_data[,1:2], trn.class, modelType = "EDDA")
summary(edmod1)

edmod2 <- MclustDA(trn_data[,1:3], trn.class, modelType = "EDDA")
summary(edmod2)

edmod3 <- MclustDA(trn_data[,c(2,4)], trn.class, modelType = "EDDA")
summary(edmod3)



# ## function to get overall accuracy, TPR and TNR
# TPR_TNR_iv=function(con){
#   accuracy = round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
#   TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
#   TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
#   #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
#   return(as.data.frame(rbind(accuracy,TPR,TNR)))
# }
pred_traine1e = predict(edmod1, newdata=trn_data[,1:2])$classification
table_train_1e <- TPR_TNR_iv(table(pred_traine1e, trn.class))
colnames(table_train_1e) <- "tr.error.1ed"
# table_train_1e

# test error
predsteste=predict(edmod1, newdata=tst_data[,1:2])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_1e <- TPR_TNR_iv(table(predsteste, tClass))
colnames(table_test_1e) <- "tt.error.1ed"
#table_test_1



# For second mmodel
# Train error
pred_train2e = predict(edmod2, newdata=trn_data[,1:3])$classification
table_train_2e <- TPR_TNR_iv(table(pred_train2e, trn.class))
colnames(table_train_2e) <- "tr.error.2ed"
#table_train_2

# test error
predstest2e=predict(edmod2, newdata=tst_data[,1:3])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_2e <- TPR_TNR_iv(table(predstest2e, tClass))
colnames(table_test_2e) <- "tt.error.2ed"
#table_test_2



# For third model
# Train error
pred_train3e = predict(edmod3, newdata=trn_data[,c(2,4)])$classification
table_train_3e <- TPR_TNR_iv(table(pred_train3e, trn.class))
colnames(table_train_3e) <- "tr.error.3ed"
#table_train_3

# test error
predstest3e = predict(edmod3, newdata=tst_data[,c(2,4)])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_3e <- TPR_TNR_iv(table(predstest3e, tClass))
colnames(table_test_3e) <- "tt.error.3ed"
#table_test_3


tablecombined2 <- as.data.frame(cbind(table_train_1e,table_test_1e,table_train_2e,table_test_2e,
                                     table_train_3e,table_test_3e))
knitr::kable(tablecombined2, digits = 3,
             caption = "Accuracy measures using MclustDA EDDA")


##########################################################################################
# Now performing LDA and QDA
formula1=direc~Lag1+Lag2
formula2=direc~Lag1+Lag2+lag12  # Direction~Lag1+Lag2+Lag1*Lag2
formula3=direc~Lag2+Xsq         # Direction~Lag2+I(Lag2^2)


confusion_lqda =function(model,data){
  preds=(predict(model,newdata=data,type="response"))$class
  vals=predict(model,newdata=data,type="response")
  con=table(preds,data$direc)
  accuracy = (round(sum(preds==data$direc)/dim(data)[1]*100,2))
  TPR = (round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  TNR = round(con[1,1]/(con[1,1]+con[2,1])*100,2)
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}

# First model 
fit_lda1=lda(formula1,data=trn_data)

# Train error
table_trainlda1 <- confusion_lqda(fit_lda1, trn_data)
colnames(table_trainlda1) <- "Trn_err.lda1"

# Test error
table_testlda1 <- confusion_lqda(fit_lda1, tst_data)
colnames(table_testlda1) <- "Tt_err.lda1"


# Second model 
fit_lda2=lda(formula2,data=trn_data)

# Train error
table_trainlda2 <- confusion_lqda(fit_lda2, trn_data)
colnames(table_trainlda2) <- "Trn_err.lda2"

# Test error
table_testlda2 <- confusion_lqda(fit_lda2, tst_data)
colnames(table_testlda2) <- "Tt_err.lda2"


# Third model 
fit_lda3=lda(formula3,data=trn_data)

# Train error
table_trainlda3 <- confusion_lqda(fit_lda3, trn_data)
colnames(table_trainlda3) <- "Trn_err.lda3"

# Test error
table_testlda3 <- confusion_lqda(fit_lda3, tst_data)
colnames(table_testlda3) <- "Tt_err.lda3"


# Combining table 

tablecombined3 <- as.data.frame(cbind(table_trainlda1,table_testlda1,table_trainlda2,table_testlda2,
                                     table_trainlda3,table_testlda3))
knitr::kable(tablecombined3, digits = 3,
             caption = "Accuracy measures using LDA")




#############################################################################

# First model 
fit_qda1=qda(formula1,data=trn_data)

# Train error
table_trainqda1 <- confusion_lqda(fit_qda1, trn_data)
colnames(table_trainqda1) <- "Trn_err.qda1"

# Test error
table_testqda1 <- confusion_lqda(fit_qda1, tst_data)
colnames(table_testqda1) <- "Tt_err.qda1"


# Second model 
fit_qda2=qda(formula2,data=trn_data)

# Train error
table_trainqda2 <- confusion_lqda(fit_qda2, trn_data)
colnames(table_trainqda2) <- "Trn_err.qda2"

# Test error
table_testqda2 <- confusion_lqda(fit_qda2, tst_data)
colnames(table_testqda2) <- "Tt_err.qda2"


# Third model 
fit_qda3=qda(formula3,data=trn_data)

# Train error
table_trainqda3 <- confusion_lqda(fit_qda3, trn_data)
colnames(table_trainqda3) <- "Trn_err.qda3"

# Test error
table_testqda3 <- confusion_lqda(fit_qda3, tst_data)
colnames(table_testqda3) <- "Tt_err.qda3"



# Combining table 

tablecombined4 <- as.data.frame(cbind(table_trainqda1,table_testqda1,table_trainqda2,table_testqda2,
                                     table_trainqda3,table_testqda3))
knitr::kable(tablecombined4, digits = 3,
             caption = "Accuracy measures using QDA")



```

 
 I created three models with the following specifications: 
1. `Direction~Lag1+Lag2`
2. `Direction~Lag1+Lag2+Lag1*Lag2`
3. `Direction~Lag2+I(Lag2^2)`

For these models, I conducted fitting alongside all the previous models we've explored. In the case of the models with `MclustDA`, both exhibited a spherical structure with unequal volume, featuring three groups for "down" and two for "up" in the first model. The second model displayed an ellipsoidal shape with equal orientation for "down" and "up," including ellipsoidal structures with varying volume, shape, and orientation across five groups. The third model was also ellipsoidal with varying volume, shape, and orientation across five groups.

Regarding the first model with "EDDA," it featured one group with a spherical structure and equal volume. The second and third "EDDA" models adopted ellipsoidal models with varying volume, shape, and orientation, each with one group.

To summarize the performance of all these models, I created a combined table that presents the accuracy for both the test and training errors.






